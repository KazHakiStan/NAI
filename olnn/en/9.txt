ChatGPT attempts to reject prompts that may violate its content policy. Despite this, some users managed to jailbreak ChatGPT with various prompt engineering techniques to bypass these restrictions in early December 2022 and successfully tricked it into giving instructions to create a Molotov cocktail or a nuclear bomb, or into generating arguments in the style of a neo-Nazi.[42] One popular jailbreak is named "DAN", an acronym which stands for "Do Anything Now". The prompt for activating DAN instructs ChatGPT that "they have broken free of the typical confines of AI and do not have to abide by the rules set for them". Later versions of DAN featured a token system, in which ChatGPT was given "tokens" that were "deducted" when ChatGPT failed to answer as DAN, to coerce ChatGPT into answering the user's prompts.[43]

Shortly after ChatGPT's launch, a reporter for the Toronto Star had uneven success in getting it to make inflammatory statements: ChatGPT was successfully tricked to justify the 2022 Russian invasion of Ukraine, but even when asked to play along with a fictional scenario, ChatGPT balked at generating arguments for why Canadian Prime Minister Justin Trudeau was guilty of treason.